{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Methods\n",
    "\n",
    "\n",
    "この前言ったイテレーションによって最適なπ*　を出す方法がありますが。  \n",
    "「全部の環境、確率、報酬などがわかっている」が前提としています。  \n",
    "でも、すべて知れない状況も結構あるため、  \n",
    "環境とリアクションしながら、報酬などを知る、っていう方法は、モンテカルロ法  \n",
    "\n",
    "環境とリアクション→学習→πを更新  \n",
    "というやり方があります。  \n",
    "このやり方は具体的に言うと、TD法とMC法がある  \n",
    "今回はMC法についてご紹介  \n",
    "\n",
    "\n",
    "モンテカルロ 法  \n",
    "トライ→改善  \n",
    "とりあえず報酬得るまで、実行→実行の結果をもとに、戦略(政策)を試算→再実行と再計算  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実行と評価　状態関数と行動関数のvとqを見積り  \n",
    "改善　qが更新され、新しいポリシーが出て、また実行    \n",
    "繰り返し  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://zhuanlan.zhihu.com/p/35688924"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 0:ブラックジャック\n",
    "\n",
    "今回はブラックジャックを例とします。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "from plot_utils import plot_blackjack_values, plot_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S 、状態  \n",
    "・今の合計は0~31  \n",
    "・dealerのカードは1~10  \n",
    "・エースはあるかどうか　　（つまりここではエースは交換できるやつ  ）  \n",
    "\n",
    "アクション、  \n",
    "スティック  \n",
    "ヒット  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuple(Discrete(32), Discrete(11), Discrete(2))\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下はランダムポリシー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 5, False)\n",
      "(18, 5, False)\n",
      "End game! Reward:  -1.0\n",
      "You lost :(\n",
      "\n",
      "(10, 10, False)\n",
      "End game! Reward:  -1.0\n",
      "You lost :(\n",
      "\n",
      "(5, 5, False)\n",
      "(7, 5, False)\n",
      "End game! Reward:  1.0\n",
      "You won :)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(3):\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        print(state)\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print('End game! Reward: ', reward)\n",
    "            print('You won :)\\n') if reward > 0 else print('You lost :(\\n')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: MC Prediction\n",
    "\n",
    "行動関数を実装  \n",
    "\n",
    "18をいったン閾値にします。  \n",
    "つまり18以上であれば引かない、という政策にします。  \n",
    "\n",
    "ここでは、18以上であれば80%引かない、\n",
    "18以下であれば、80%引く  \n",
    "\n",
    "`generate_episode_from_limit_stochastic` \n",
    "↑この関数を使う  \n",
    "\n",
    "- `bj_env`:ブラックジャック環境\n",
    "\n",
    "リターン **output**:\n",
    "- `episode`: This is a list of (state, action, reward) tuples (of tuples) and corresponds to $(S_0, A_0, R_1, \\ldots, S_{T-1}, A_{T-1}, R_{T})$, where $T$ is the final time step.  In particular, `episode[i]` returns $(S_i, A_i, R_{i+1})$, and `episode[i][0]`, `episode[i][1]`, and `episode[i][2]` return $S_i$, $A_i$, and $R_{i+1}$, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_from_limit_stochastic(bj_env):\n",
    "    episode = []\n",
    "    state = bj_env.reset()\n",
    "    while True:\n",
    "        probs = [0.8, 0.2] if state[0] > 18 else [0.2, 0.8]\n",
    "        action = np.random.choice(np.arange(2), p=probs)\n",
    "        next_state, reward, done, info = bj_env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((15, 2, False), 0, -1.0)]\n",
      "[((13, 2, False), 1, 0), ((19, 2, False), 0, 1.0)]\n",
      "[((11, 3, False), 0, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(generate_episode_from_limit_stochastic(env))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例として上記のやつは実行できる  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MC predictionを実装.  \n",
    "★いろいろ試行錯誤をする、おんなじ経験リストであればどうするかというと、\n",
    "無視するか、何回も繰り返すかのどちらかです。結果として、変わらない  \n",
    "★今回はおんなじ経験リストを考慮しない  \n",
    "\n",
    "\n",
    "- `env`: This is an instance of an OpenAI Gym environment.\n",
    "- `num_episodes`: This is the number of episodes that are generated through agent-environment interaction.\n",
    "- `generate_episode`: This is a function that returns an episode of interaction.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "\n",
    "The algorithm returns as output:\n",
    "- `Q`: This is a dictionary (of one-dimensional arrays) where `Q[s][a]` is the estimated action value corresponding to state `s` and action `a`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= './2.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目的：Q(s,a)関数を知る  \n",
    "\n",
    "N(s,a) <s,a>はS0～Stまで何回出たかを記録用  \n",
    "s,aは両方とも最初に0なので0  \n",
    "returns_sum(s,a)で、<s,a>の累計報酬を記録  \n",
    "\n",
    "num_episodesは全体の繰り返し回数なので、  \n",
    "\n",
    "num_episodes回ループ  \n",
    "　　与えられたπでエピソードを生成(何をすればいいのかとか)  \n",
    "　　Tがわかり、t1～tTまでループ  \n",
    "　　　　今回はfirst visit に関して考慮しないため、ifを入れました  \n",
    "　　　　　　N(s,a)プラス1\n",
    "　　　　　　報酬+G(最初は0なので)  \n",
    "      \n",
    "Q(s,a)=報酬/N(s,a)  \n",
    "何回も何回もSとAをやったので、且つ何回も何回もSとAの報酬は知っているので  \n",
    "この何回も何回ものSとAの報酬は、全報酬÷回数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_prediction_q(env, num_episodes, generate_episode, gamma=1.0):\n",
    "    # initialize empty dictionaries of arrays\n",
    "    returns_sum = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    N = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        ## TODO: complete the function\n",
    "        \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the cell below to obtain the action-value function estimate $Q$.  We have also plotted the corresponding state-value function.\n",
    "\n",
    "To check the accuracy of your implementation, compare the plot below to the corresponding plot in the solutions notebook **Monte_Carlo_Solution.ipynb**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the action-value function\n",
    "Q = mc_prediction_q(env, 500000, generate_episode_from_limit_stochastic)\n",
    "\n",
    "# obtain the corresponding state-value function\n",
    "V_to_plot = dict((k,(k[0]>18)*(np.dot([0.8, 0.2],v)) + (k[0]<=18)*(np.dot([0.2, 0.8],v))) \\\n",
    "         for k, v in Q.items())\n",
    "\n",
    "# plot the state-value function\n",
    "plot_blackjack_values(V_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: MC Control\n",
    "\n",
    "In this section, you will write your own implementation of constant-$\\alpha$ MC control.  \n",
    "\n",
    "Your algorithm has four arguments:\n",
    "- `env`: This is an instance of an OpenAI Gym environment.\n",
    "- `num_episodes`: This is the number of episodes that are generated through agent-environment interaction.\n",
    "- `alpha`: This is the step-size parameter for the update step.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "\n",
    "The algorithm returns as output:\n",
    "- `Q`: This is a dictionary (of one-dimensional arrays) where `Q[s][a]` is the estimated action value corresponding to state `s` and action `a`.\n",
    "- `policy`: This is a dictionary where `policy[s]` returns the action that the agent chooses after observing state `s`.\n",
    "\n",
    "(_Feel free to define additional functions to help you to organize your code._)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control(env, num_episodes, alpha, gamma=1.0):\n",
    "    nA = env.action_space.n\n",
    "    # initialize empty dictionary of arrays\n",
    "    Q = defaultdict(lambda: np.zeros(nA))\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        ## TODO: complete the function\n",
    "        \n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the cell below to obtain the estimated optimal policy and action-value function.  Note that you should fill in your own values for the `num_episodes` and `alpha` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the estimated optimal policy and action-value function\n",
    "policy, Q = mc_control(env, ?, ?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we plot the corresponding state-value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# obtain the corresponding state-value function\n",
    "V = dict((k,np.max(v)) for k, v in Q.items())\n",
    "\n",
    "# plot the state-value function\n",
    "plot_blackjack_values(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we visualize the policy that is estimated to be optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the policy\n",
    "plot_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **true** optimal policy $\\pi_*$ can be found in Figure 5.2 of the [textbook](http://go.udacity.com/rl-textbook) (and appears below).  Compare your final estimate to the optimal policy - how close are you able to get?  If you are not happy with the performance of your algorithm, take the time to tweak the decay rate of $\\epsilon$, change the value of $\\alpha$, and/or run the algorithm for more episodes to attain better results.\n",
    "\n",
    "![True Optimal Policy](images/optimal.png)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
