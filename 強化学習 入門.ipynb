{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 強化学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://blog.brainpad.co.jp/entry/2017/02/24/121500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "強化学習では、与えられた「環境」における価値（あるいは「利益」と呼びます）を最大化するように「エージェント」を学習させます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "状態s　行動a　報酬r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s∊S　a∊A　r∊R  \n",
    "Q 状態行動価値 Q(s,a)  \n",
    "Q値は「報酬」ではなく「価値」である、つまり全体的に長期的な価値  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q(st,at)=Est+1 (rt+1 + γEat+1(Q(st+1,at+1)))  \n",
    "わかりずらい！  \n",
    "Q(st,at)　　=Est+1 (rt+1 + γEat+1(Q(st+1,at+1)))  　　　\n",
    "t時点の価値 =t1時点　Eは期待値、　Esはsに関する期待値、Eaはaに関する期待値  \n",
    "rは報酬、γは割引率  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q学習  \n",
    "    Q(st,at)←(1−α)Q(st,at)+α(rt+1+γMAXat+1 Q(st+1,at+1))  \n",
    "    αは学習率  \n",
    "    1-αは、今のQ  α将来のQ、要は全部現在や将来に注目するんじゃなくて、調和的に最大化を図ること  \n",
    "    最大値を見積もって行動することです。  \n",
    "Sarsa  \n",
    "　　Q(st,at)←(1−α)Q(st,at)+α(rt+1+γQ(st+1,at+1))\n",
    "  　MAXはいらない、実際行動して得た報酬にします。  \n",
    "    上のQ学習は、行動する前に、最大値を見積もって行動することです。  \n",
    "    ここが違い  \n",
    "モンテカルロ 法  \n",
    "    「とにかく何らかの報酬が得られるまで行動をしてみて、\n",
    "    その報酬値を知ってから、辿ってきた状態と行動に対してその報酬を分配していきます。」  \n",
    "    1,Returns(s,a) ← append (Returns(s,a),r)\n",
    "    2,Q(s,a)←average(Returns(s,a))\n",
    "    1は、すべての行動と状態から得られた報酬を収集  \n",
    "    2は、その報酬の平均値をとる  \n",
    "    このやり方は、長期的な視点で見るのがいいかも  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サイトまとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://qiita.com/eve_yk/items/2ace6d4c1dad7e912df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "強化学習のモデル化(Markov Decision Process)  \n",
    "States: S　状況。  \n",
    "\n",
    "Model: T(s, a, s') (=P(s'|s, a))\n",
    "　TはTransitionで、状況sの時に行動aをとると状況s'になる、ということ。ただし、aを選択しても発動しなかったり別のところに行ったり、といった状況を表現するため確率的な表現(P(s'|s, a))になる  \n",
    "\n",
    "Actions: A(s), A 行動  \n",
    "\n",
    "Reward: R(s), R(s, a), R(s, a , s')\n",
    "\n",
    "Policy: π(s)　π(s) -> a戦略。\n",
    "状況sにおいてどういう行動aを取るべきか、\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uπ(s)=E[∑t∞γtR(st)|π,s0=s]  \n",
    "これもわかりずらい！！  \n",
    "Uは報酬の総和\n",
    "Uπ(s)は、この状態で、この戦略を行う場合の報酬の総和  \n",
    "Σの後は、γは時間に対する割引、Rは報酬  \n",
    "なのでなので、\n",
    "最適な戦略は  \n",
    "π∗(s)=argmaxa∑s′T(s,a,s′)U(s′)  \n",
    "書き換えると  \n",
    "U(s)=R(s)+γmax∑s′T(s,a,s′)U(s′)  \n",
    "これは、Bellman Equationと呼ばれる  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value Iteration  \n",
    "環境のみから最適行動計算  \n",
    "\n",
    "　1、確定報酬を設定  \n",
    "　2、各状態sについて、実行可能なaにより得られる報酬を計算する  \n",
    "　3、γ∑T(s,a,s′)U(s′)   \n",
    "　4、1の中で最も高い報酬が得られるaで、報酬総和U(s)U(s)を計算する   \n",
    "　5、U(s)=R(s)+γmax∑s′T(s,a,s′)U(s′)　　　(Bellman Equation)  \n",
    "　6、収束する(U(s)U(s)の更新幅が少なくなるまで)まで1に戻り、更新を繰り返す  \n",
    "　7、最終的には期待値へ収束することが証明されている    \n",
    " すべての状況のすべての行動を試しつくし方法  \n",
    "Policy Iteration  \n",
    "　1、適当な戦略(π0)を決める  \n",
    "　2、戦略を元にUπt(s)を計算する  \n",
    "　3、戦略πtを更新しπt+1とする。  \n",
    " 　　πt+1=argmaxa∑T(s,a,s′)Uπt(s′))  \n",
    "　4、収束するまで1に戻り、更新を繰り返す  \n",
    "\n",
    "一個目は環境ベースに更新、二個目は戦略(policy)ベースに更新  \n",
    "\n",
    "二つのイテレーションに関して、 \n",
    "T(s,a,s')っていうのがあり、つまり事前に遷移先明らかにしなければならないです。  \n",
    "\n",
    "\n",
    "Q-Learning は事前の環境設定は不要  \n",
    "\n",
    "Q-learning  \n",
    "　さっき勉強しました。  \n",
    " 　まず試してみる  \n",
    "　　で試行錯誤  \n",
    "  \n",
    "  Q(s,a)≈R(s,a)+γmaxa′E[Q(s′,a′)]  \n",
    "  \n",
    "  さっき学びましたが、　今回の報酬は、今回の報酬+次回最大報酬 言い方変ですけど  \n",
    "  ≒の意味は、Q(s′,a′)はあくまでも見積りで、みつもりは必ずしも正しいとは限らないです。\n",
    "  \n",
    "  Q(s,a)=Q(s,a)+α(R(s,a)+γmaxa′E[Q(s′,a′)]−Q(s,a))  \n",
    "  \n",
    "  Q(s,a)=(1−α)Q(s,a)+α(R(s,a)+γmaxa′E[Q(s′,a′)])  \n",
    "  \n",
    "  \n",
    "Deep Q-learning  \n",
    "Qlearning+DL  \n",
    "   Q(s,a)をニューラルネット化  \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 理論的な入門\n",
    "https://www.slideshare.net/mitmul/ss-20417728"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3python出始める強化学習\n",
    "https://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
